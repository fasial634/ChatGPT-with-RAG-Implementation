{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install libraries\n!pip install openai \n!pip install langchain-openai\n!pip install langchain\n!pip install langchain-community\n!pip install qdrant-client","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:17:17.300645Z","iopub.execute_input":"2024-07-23T07:17:17.301092Z","iopub.status.idle":"2024-07-23T07:17:17.306079Z","shell.execute_reply.started":"2024-07-23T07:17:17.301059Z","shell.execute_reply":"2024-07-23T07:17:17.304872Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"import os \nfrom langchain_openai import ChatOpenAI\nfrom openai import OpenAI","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:33:38.275889Z","iopub.execute_input":"2024-07-23T06:33:38.276437Z","iopub.status.idle":"2024-07-23T06:33:38.281929Z","shell.execute_reply.started":"2024-07-23T06:33:38.276402Z","shell.execute_reply":"2024-07-23T06:33:38.280662Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Without using RAG","metadata":{}},{"cell_type":"code","source":"model = ChatOpenAI(\n    api_key=\"\",\n    model = \"gpt-3.5-turbo-0125\", \n    temperature = 0.7, \n    max_tokens = 512, \n    max_retries = 2, \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:33:40.364659Z","iopub.execute_input":"2024-07-23T06:33:40.365068Z","iopub.status.idle":"2024-07-23T06:33:40.405009Z","shell.execute_reply.started":"2024-07-23T06:33:40.365034Z","shell.execute_reply":"2024-07-23T06:33:40.403845Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from langchain.schema import SystemMessage, HumanMessage,AIMessage\n\ntext = [\n    SystemMessage(content=\"You are an AI assistant.\"),\n    HumanMessage(content=\"Explain AI technology\"),\n    AIMessage(content=\"Model number of parameters\"),\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:33:42.601451Z","iopub.execute_input":"2024-07-23T06:33:42.601941Z","iopub.status.idle":"2024-07-23T06:33:42.613179Z","shell.execute_reply.started":"2024-07-23T06:33:42.601900Z","shell.execute_reply":"2024-07-23T06:33:42.611918Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"text.append(HumanMessage(content=\"What is so special about Mistral 7B?\")) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = model.invoke(text)\nprint(response.content)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:35:05.064063Z","iopub.execute_input":"2024-07-23T06:35:05.064815Z","iopub.status.idle":"2024-07-23T06:35:07.642505Z","shell.execute_reply.started":"2024-07-23T06:35:05.064783Z","shell.execute_reply":"2024-07-23T06:35:07.641488Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"AI, or artificial intelligence, is a branch of computer science that involves creating computer systems capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI technologies are designed to simulate human cognitive functions, learn from data, and adapt to new information or stimuli.\n\nThere are several types of AI technologies, including machine learning, deep learning, natural language processing, computer vision, and robotic process automation. These technologies enable computers to analyze large amounts of data, recognize patterns, make decisions, and carry out tasks autonomously.\n\nOverall, AI technology aims to improve efficiency, accuracy, and automation in various industries and applications, such as healthcare, finance, transportation, cybersecurity, and customer service. It has the potential to revolutionize many aspects of our daily lives and drive innovation in numerous fields.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Adding Context ","metadata":{}},{"cell_type":"code","source":"# adding context with the question\ninformation_about_Mistral = [\n    \"Unlike older models, Mistral 7B can grasp intricate details and context in language. This allows it to interpret and respond with more human-like fluency and coherence.\",\n    \"Long attention span: Mistral 7B can consider a whopping 8,192 tokens of text, which is significantly longer than most models. This enables it to analyze broader contexts and generate more relevant responses.\",\n    \"Efficient memory usage:  To handle such a long context without ballooning memory usage, Mistral 7B utilizes a unique rolling buffer cache. This cache only retains a set amount of past information, like a carousel with limited seats, ensuring efficient memory management\", \n    \"Specialized attention: Traditional models allow every word to attend to every other preceding word. Mistral 7B implements a sliding window attention where each word focuses on a specific window of preceding words. This streamlines processing without sacrificing accuracy.\",\n    \"Fine-tuning capabilities: The base Mistral 7B model excels at understanding language. Additionally, it can be further customized for specific tasks by fine-tuning it on relevant datasets. For instance, Mistral 7B Instruct is fine-tuned for following instructions and achieves impressive results.\", \n    \"Safety features: While powerful, Mistral 7B incorporates safety features like prompting and content moderation. Prompting allows users to guide the model's output towards a desired outcome, while moderation helps prevent potentially harmful content generation.\", \n    \"Overall, Mistral 7B's ability to understand complex language, handle long contexts efficiently, and be fine-tuned for specific tasks makes it a significant advancement in NLP. Its safety features further ensure responsible use of this powerful technology.\"\n]\n\nknowledge = \"\\n\".join(information_about_Mistral)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:50:49.085181Z","iopub.execute_input":"2024-07-23T06:50:49.085596Z","iopub.status.idle":"2024-07-23T06:50:49.093052Z","shell.execute_reply.started":"2024-07-23T06:50:49.085565Z","shell.execute_reply":"2024-07-23T06:50:49.091957Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(source_knowledge)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:50:51.910591Z","iopub.execute_input":"2024-07-23T06:50:51.911294Z","iopub.status.idle":"2024-07-23T06:50:51.917594Z","shell.execute_reply.started":"2024-07-23T06:50:51.911260Z","shell.execute_reply":"2024-07-23T06:50:51.916477Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1020"},"metadata":{}}]},{"cell_type":"code","source":"text = [\n    SystemMessage(content=\"You are an AI assistant.\"),\n    HumanMessage(content=\"Explain AI technology\"),\n    AIMessage(content=\"Model number of parameters\"),\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:50:54.259377Z","iopub.execute_input":"2024-07-23T06:50:54.259811Z","iopub.status.idle":"2024-07-23T06:50:54.265760Z","shell.execute_reply.started":"2024-07-23T06:50:54.259780Z","shell.execute_reply":"2024-07-23T06:50:54.264515Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"question = \"What is so special about Mistral 7B?\"\n\nprompt = f\"\"\"Using the contexts below to answer the question.\n\nContexts:\n{knowledge}\n\nQuestion: {question}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:51:01.107602Z","iopub.execute_input":"2024-07-23T06:51:01.108023Z","iopub.status.idle":"2024-07-23T06:51:01.113961Z","shell.execute_reply.started":"2024-07-23T06:51:01.107984Z","shell.execute_reply":"2024-07-23T06:51:01.112665Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"full_prompt = HumanMessage(\n    content=prompt\n)\n\ntext.append(full_prompt)\n\nresponse = model.invoke(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:51:28.839850Z","iopub.execute_input":"2024-07-23T06:51:28.840254Z","iopub.status.idle":"2024-07-23T06:51:32.162252Z","shell.execute_reply.started":"2024-07-23T06:51:28.840222Z","shell.execute_reply":"2024-07-23T06:51:32.161067Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# as shown in the response above, the sytem provider model is acble of definiing the feature of Mistral 7B better \nprint(response.content)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:51:33.834613Z","iopub.execute_input":"2024-07-23T06:51:33.835640Z","iopub.status.idle":"2024-07-23T06:51:33.840813Z","shell.execute_reply.started":"2024-07-23T06:51:33.835602Z","shell.execute_reply":"2024-07-23T06:51:33.839704Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Mistral 7B stands out due to its ability to grasp intricate details and context in language, enabling it to respond with human-like fluency and coherence. It has a long attention span, capable of considering 8,192 tokens of text, allowing it to analyze broader contexts and generate more relevant responses. The model efficiently manages memory usage through a unique rolling buffer cache, ensuring optimal performance. Mistral 7B's specialized attention mechanism uses a sliding window approach, enhancing processing efficiency without compromising accuracy. Additionally, its fine-tuning capabilities enable customization for specific tasks, enhancing performance in various applications. The model also incorporates safety features like prompting and content moderation to ensure responsible and safe use of its powerful technology. Overall, Mistral 7B's advanced language understanding, efficient handling of long contexts, and adaptability for specific tasks make it a significant advancement in Natural Language Processing (NLP).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Implementing Retrieval Augmented Generation (RAG)","metadata":{}},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"# This dataset consists of selected portions from the Mistral 7B research paper.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n\nprint(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T06:59:40.300445Z","iopub.execute_input":"2024-07-23T06:59:40.300853Z","iopub.status.idle":"2024-07-23T06:59:43.727849Z","shell.execute_reply.started":"2024-07-23T06:59:40.300822Z","shell.execute_reply":"2024-07-23T06:59:43.726773Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{'doi': '2310.06825', 'chunk-id': '0', 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', 'id': '2310.06825', 'title': 'Mistral 7B', 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.', 'source': 'http://arxiv.org/pdf/2310.06825', 'authors': ['Albert Q. Jiang', 'Alexandre Sablayrolles', 'Arthur Mensch', 'Chris Bamford', 'Devendra Singh Chaplot', 'Diego de las Casas', 'Florian Bressand', 'Gianna Lengyel', 'Guillaume Lample', 'Lucile Saulnier', 'Lélio Renard Lavaud', 'Marie-Anne Lachaux', 'Pierre Stock', 'Teven Le Scao', 'Thibaut Lavril', 'Thomas Wang', 'Timothée Lacroix', 'William El Sayed'], 'categories': ['cs.CL', 'cs.AI', 'cs.LG'], 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/', 'journal_ref': None, 'primary_category': 'cs.CL', 'published': '20231010', 'updated': '20231010', 'references': [{'id': '1808.07036'}, {'id': '1809.02789'}, {'id': '1904.10509'}, {'id': '2302.13971'}, {'id': '2009.03300'}, {'id': '2305.13245'}, {'id': '1904.09728'}, {'id': '1803.05457'}, {'id': '2103.03874'}, {'id': '1905.07830'}, {'id': '2308.12950'}, {'id': '2210.09261'}, {'id': '2310.06825'}, {'id': '2307.09288'}, {'id': '2304.06364'}, {'id': '1905.10044'}, {'id': '2110.14168'}, {'id': '2108.07732'}, {'id': '2107.03374'}, {'id': '1811.00937'}, {'id': '2004.05150'}, {'id': '1705.03551'}]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# convert it into a dataframe to more readable\ndata = dataset.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:02:20.802116Z","iopub.execute_input":"2024-07-23T07:02:20.802907Z","iopub.status.idle":"2024-07-23T07:02:20.816803Z","shell.execute_reply.started":"2024-07-23T07:02:20.802872Z","shell.execute_reply":"2024-07-23T07:02:20.815652Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"chunks = data[[\"chunk\", \"source\"]]\nchunks.head(4)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:08:28.381163Z","iopub.execute_input":"2024-07-23T07:08:28.381582Z","iopub.status.idle":"2024-07-23T07:08:28.393938Z","shell.execute_reply.started":"2024-07-23T07:08:28.381550Z","shell.execute_reply":"2024-07-23T07:08:28.392686Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                                               chunk  \\\n0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n1  automated benchmarks. Our models are released ...   \n2  GQA significantly accelerates the inference sp...   \n3  Mistral 7B takes a significant step in balanci...   \n\n                            source  \n0  http://arxiv.org/pdf/2310.06825  \n1  http://arxiv.org/pdf/2310.06825  \n2  http://arxiv.org/pdf/2310.06825  \n3  http://arxiv.org/pdf/2310.06825  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n      <td>http://arxiv.org/pdf/2310.06825</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>automated benchmarks. Our models are released ...</td>\n      <td>http://arxiv.org/pdf/2310.06825</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GQA significantly accelerates the inference sp...</td>\n      <td>http://arxiv.org/pdf/2310.06825</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mistral 7B takes a significant step in balanci...</td>\n      <td>http://arxiv.org/pdf/2310.06825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from langchain_community.document_loaders import DataFrameLoader\n\nloader = DataFrameLoader(chunks, page_content_column=\"chunk\")\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:08:56.560534Z","iopub.execute_input":"2024-07-23T07:08:56.561547Z","iopub.status.idle":"2024-07-23T07:08:56.575877Z","shell.execute_reply.started":"2024-07-23T07:08:56.561510Z","shell.execute_reply":"2024-07-23T07:08:56.574735Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"len(documents)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:09:01.568944Z","iopub.execute_input":"2024-07-23T07:09:01.569361Z","iopub.status.idle":"2024-07-23T07:09:01.576547Z","shell.execute_reply.started":"2024-07-23T07:09:01.569314Z","shell.execute_reply":"2024-07-23T07:09:01.575276Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}]},{"cell_type":"code","source":"print(documents[0].metadata)\nprint(documents[0].page_content)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:10:30.162475Z","iopub.execute_input":"2024-07-23T07:10:30.162896Z","iopub.status.idle":"2024-07-23T07:10:30.169119Z","shell.execute_reply.started":"2024-07-23T07:10:30.162864Z","shell.execute_reply":"2024-07-23T07:10:30.167956Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"{'source': 'http://arxiv.org/pdf/2310.06825'}\nMistral 7B\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\nWilliam El Sayed\nAbstract\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\nautomated benchmarks. Our models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.vectorstores import Qdrant\nfrom langchain_openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"\",)\n\nqdrant = Qdrant.from_documents(\n    documents=documents,\n    embedding=embeddings,\n    url=\"\",\n    collection_name=\"chatbot\",\n    api_key= \"\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:13:15.072858Z","iopub.execute_input":"2024-07-23T07:13:15.073371Z","iopub.status.idle":"2024-07-23T07:13:25.114835Z","shell.execute_reply.started":"2024-07-23T07:13:15.073307Z","shell.execute_reply":"2024-07-23T07:13:25.113873Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Finding the chunks that are similar to the question.\nquery = \"What is so special about Mistral 7B?\"\nqdrant.similarity_search(query, k=3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:13:39.035538Z","iopub.execute_input":"2024-07-23T07:13:39.035953Z","iopub.status.idle":"2024-07-23T07:13:40.236563Z","shell.execute_reply.started":"2024-07-23T07:13:39.035921Z","shell.execute_reply":"2024-07-23T07:13:40.235402Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '0c984efe-db82-47dc-b9f5-73b9d3ff6e6c', '_collection_name': 'chatbot'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '00832a52-d75d-4a10-b711-8f85bb2f7600', '_collection_name': 'chatbot'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '8e588d05-626b-4d98-80f0-6c14a854e797', '_collection_name': 'chatbot'}, page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more')]"},"metadata":{}}]},{"cell_type":"code","source":"def custom_prompt(query: str):\n    results = qdrant.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n\n    Contexts:\n    {source_knowledge}\n\n    Query: {query}\"\"\"\n    return augment_prompt","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:14:28.792671Z","iopub.execute_input":"2024-07-23T07:14:28.793074Z","iopub.status.idle":"2024-07-23T07:14:28.799907Z","shell.execute_reply.started":"2024-07-23T07:14:28.793036Z","shell.execute_reply":"2024-07-23T07:14:28.798544Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(custom_prompt(query))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:14:32.287613Z","iopub.execute_input":"2024-07-23T07:14:32.288037Z","iopub.status.idle":"2024-07-23T07:14:33.594427Z","shell.execute_reply.started":"2024-07-23T07:14:32.288005Z","shell.execute_reply":"2024-07-23T07:14:33.593357Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Using the contexts below, answer the query:\n\n    Contexts:\n    Mistral 7B\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\nWilliam El Sayed\nAbstract\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\nautomated benchmarks. Our models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\nMistral 7B\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\nWilliam El Sayed\nAbstract\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\nautomated benchmarks. Our models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\nlarge language models efficient. Through our work, our aim is to help the community create more\n\n    Query: What is so special about Mistral 7B?\n","output_type":"stream"}]},{"cell_type":"code","source":"text = [\n    SystemMessage(content=\"You are an AI assistant.\"),\n    HumanMessage(content=\"Explain AI technology\"),\n    AIMessage(content=\"Model number of parameters\"),\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:15:26.065691Z","iopub.execute_input":"2024-07-23T07:15:26.066160Z","iopub.status.idle":"2024-07-23T07:15:26.072726Z","shell.execute_reply.started":"2024-07-23T07:15:26.066124Z","shell.execute_reply":"2024-07-23T07:15:26.071488Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"prompt = HumanMessage(\n    content=custom_prompt(query)\n)\n\ntext.append(prompt)\n\nreponse = model.invoke(text)\n\nprint(reponse.content)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T07:15:44.577375Z","iopub.execute_input":"2024-07-23T07:15:44.578482Z","iopub.status.idle":"2024-07-23T07:15:48.303942Z","shell.execute_reply.started":"2024-07-23T07:15:44.578442Z","shell.execute_reply":"2024-07-23T07:15:48.302625Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Mistral 7B is a 7-billion-parameter language model that stands out for its superior performance and efficiency. It surpasses other models such as the best open 13B model (Llama 2) and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation across various benchmarks. Mistral 7B leverages innovative attention mechanisms like grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length efficiently with reduced inference costs. These attention mechanisms contribute to Mistral 7B's enhanced performance and efficiency. Additionally, Mistral 7B is designed for ease of fine-tuning across a wide range of tasks and is accompanied by a reference implementation for easy deployment on various platforms, making it adaptable and high-performing.\n","output_type":"stream"}]}]}